{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flaviocrispin/twitter_analisys/blob/main/2_tratamento_e_limpeza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj6erTigpV2w"
      },
      "source": [
        "# Tratamento e limpeza dos dados\n",
        "- Carregamento dos dados\n",
        "- Exclusão de caracteres especiais (@:// e etc)\n",
        "- Stopwords\n",
        "- Conversão de emojis e emoticons em sentimentos\n",
        "- Stemmed\n",
        "- Tokenização\n",
        "- Salvar o novo DATAFRAME no google drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwWCxaswQTbI"
      },
      "source": [
        "## 1. Exploração dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZnUIO7-LlgV",
        "outputId": "3bbc2451-de8f-4760-d595-de0c0f843961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect google colab with google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "aYT9CuHrLxqk"
      },
      "outputs": [],
      "source": [
        "#@title Libraries imports\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENnVPVOMfS30"
      },
      "source": [
        "### 1.1 - Transform and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "M6KYyoCInqyH"
      },
      "outputs": [],
      "source": [
        "#@title Data transform\n",
        "\n",
        "PATH_FOLDER = '/content/drive/MyDrive/Colab Notebooks/project_politics' #@param {type: 'string'}\n",
        "PATH_IMAGES = '/content/drive/MyDrive/Colab Notebooks/project_politics/img/' #@param {type: 'string'}\n",
        "PATH_MODELS = '/content/drive/MyDrive/Colab Notebooks/project_politics/models/' #@param {type: 'string'}\n",
        "PATH_DATA = '/content/drive/MyDrive/Colab Notebooks/project_politics/data/' #@param {type: 'string'}\n",
        "\n",
        "colunas = ['date','username', 'place','tweet','language','replies_count',\n",
        "           'retweets_count','likes_count','hashtags']\n",
        "\n",
        "\n",
        "def open_data (candidato):\n",
        "  dataframe = pd.read_csv(PATH_DATA + \n",
        "                          'raw/{}_tweets_extract.csv'.format(candidato),\n",
        "                          usecols = colunas)\n",
        "  df = dataframe.loc[dataframe['language'] == 'pt']\n",
        "  df['qtd_tweets'] = 1\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "  #df_data = df.loc[df['date'].between('2022-03-23', '2022-03-30')]\n",
        "  df_data = df.rename(columns={'replies_count': 'respostas', \n",
        "                                    'retweets_count': 'compartilhamento',\n",
        "                                    'likes_count': 'likes'})\n",
        "  return df_data\n",
        "\n",
        "\n",
        "def group_data_by_time (candidato):\n",
        "  dataframe = open_data (candidato).groupby(by='date')['qtd_tweets'].sum().reset_index()\n",
        "  return dataframe\n",
        "\n",
        "\n",
        "\n",
        "def data_joint (data1, data2, data3, data4):\n",
        "  df1_g = df1.groupby(by='date')['qtd_tweets'].sum().reset_index()\n",
        "  df2_g = df2.groupby(by='date')['qtd_tweets'].sum().reset_index()\n",
        "  df3_g = df3.groupby(by='date')['qtd_tweets'].sum().reset_index()\n",
        "  df4_g = df4.groupby(by='date')['qtd_tweets'].sum().reset_index()\n",
        "\n",
        "  df1_g['candidato'] = 'Bolsonaro'\n",
        "  df2_g['candidato'] = 'Lula'\n",
        "  df3_g['candidato'] = 'Ciro'\n",
        "  df4_g['candidato'] = 'Moro'\n",
        "\n",
        "  data = pd.concat([df1_g,df2_g, df3_g, df4_g],keys=['Bolsonaro','Lula', 'Ciro', 'Moro'])\n",
        "  \n",
        "  return data\n",
        "\n",
        "def data_joint_tweet_types (data1, data2, data3, data4):\n",
        "\n",
        "  data = pd.concat([data1,data2, data3, data4],keys=['Bolsonaro','Lula', 'Ciro', 'Moro'])\n",
        "\n",
        "  \n",
        "  return plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "GsEdwB5lUjtU"
      },
      "outputs": [],
      "source": [
        "#@title Load CSV files\n",
        "\n",
        "df1 = open_data('bolsonaro')\n",
        "df2 = open_data('lula')\n",
        "df3 = open_data('ciro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqJqTcGCjkv3"
      },
      "source": [
        "### 1.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anx_yrcJe-BV",
        "outputId": "93aac08b-dbda-4692-80d4-0eb50cb81c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title NLTK and EMOT install \n",
        "#import NLTK and download\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "\n",
        "\n",
        "#install EMOT library\n",
        "!pip install emot  &> /dev/null\n",
        "\n",
        "!pip install unidecode &> /dev/null\n",
        "!pip install autocorrect &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EQriVEgfkbg",
        "outputId": "a03debe4-0e16-402b-aabe-3ee4da666b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary for this language not found, downloading...\n",
            "__________________________________________________\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
          ]
        }
      ],
      "source": [
        "#@title Libraries import\n",
        "\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import string\n",
        "import unidecode\n",
        "\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "#cleaning\n",
        "import re\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# library for transform the emojis and emoticons\n",
        "import emot\n",
        "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
        "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qxv-7FZcE4l",
        "outputId": "70f5b8d8-00f1-425f-b770-ebbfb9fc80f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#@title Transform EMOJIS in sentiment\n",
        "\n",
        "# criar função para armazenar lista de emojis e emoticons\n",
        "\n",
        "# emojis = []\n",
        "# emotis = []\n",
        "# emot_obj = emot.core.emot()\n",
        "# lista_comentarios = df3['tweet'].tolist()\n",
        "\n",
        "# def localiza_emoji_emoti(text):  \n",
        "#     emoji = emot_obj.emoji(text)\n",
        "#     emoti = emot_obj.emoticons(text)\n",
        "    \n",
        "#     if emoji['flag'] == True:\n",
        "#         emojis.append(emoji['value'])\n",
        "        \n",
        "#     try:\n",
        "#         if emoti['flag'] == True:\n",
        "#             emotis.append(emoti['value'])\n",
        "#     except:\n",
        "#         emotis.append('nada')\n",
        "\n",
        "# # aplicando a função na lista de comentários\n",
        "# for txt in lista_comentarios:\n",
        "#     localiza_emoji_emoti(txt)\n",
        "\n",
        "# # removendo duplicidade dos emojis e emoticos\n",
        "# lista_emojis = []\n",
        "# lista_emotis = []\n",
        "\n",
        "# for linha in emojis:\n",
        "#     for emoji in linha:\n",
        "#         if emoji not in lista_emojis:\n",
        "#             lista_emojis.append(emoji)\n",
        "            \n",
        "# for linha in emotis:\n",
        "#     for emoti in linha:\n",
        "#         if emoti != 'nada' and emoti not in lista_emotis:\n",
        "#             lista_emotis.append(emoti)\n",
        "            \n",
        "# len(lista_emojis), len(lista_emotis)\n",
        "\n",
        "# criar dois dicionários com as interpretações, um para emojis outro para emoticons\n",
        "dict_emojis = {\n",
        "    'exclamation_question_mark': 'ruim',\n",
        "    'person_pouting': 'ruim',\n",
        "    'kiss_mark': 'ótimo',\n",
        "    'upside-down_face': 'ótimo',\n",
        "    'smiling_face_with_open_mouth_&_smiling_eyes': 'ótimo',\n",
        "    'love_letter': 'ótimo',\n",
        "    'rose': 'ótimo',\n",
        "    'angry_face_with_horns': 'ruim',\n",
        "    'yellow_heart': 'ótimo',\n",
        "    'blue_heart': 'ótimo',\n",
        "    'green_heart': 'ótimo',\n",
        "    'relieved_face': 'ótimo',\n",
        "    'trophy': 'ótimo',\n",
        "    'expressionless_face': 'ruim',\n",
        "    'slightly_smiling_face': 'ótimo',\n",
        "    'nauseated_face': 'ruim',\n",
        "    'face_with_stuck-out_tongue_&_winking_eye': 'ótimo',\n",
        "    'OK_hand': 'ótimo',\n",
        "    'neutral_face': 'ruim',\n",
        "    'person_shrugging': 'ruim',\n",
        "    'weary_face': 'ruim',\n",
        "    'heart_with_arrow': 'ótimo',\n",
        "    'grimacing_face': 'ruim',\n",
        "    'sleepy_face': 'ruim',\n",
        "    'pig_face': 'ruim',\n",
        "    'thinking_face': 'ruim',\n",
        "    'loudly_crying_face': 'ruim',\n",
        "    'blossom': 'ótimo',\n",
        "    'face_with_cold_sweat': 'ruim',\n",
        "    'crying_cat_face': 'ruim',\n",
        "    'unamused_face': 'ruim',\n",
        "    'disappointed_but_relieved_face': 'ruim',\n",
        "    'smiling_face': 'ótimo',\n",
        "    'face_screaming_in_fear': 'ruim',\n",
        "    'face_with_steam_from_nose': 'ruim',\n",
        "    'broken_heart': 'ruim',\n",
        "    'see-no-evil_monkey': 'ruim',\n",
        "    'two_hearts': 'ótimo',\n",
        "    'growing_heart': 'ótimo',\n",
        "    'slightly_frowning_face': 'ruim',\n",
        "    'crying_face': 'ruim',\n",
        "    'dizzy': 'ruim',\n",
        "    'smiling_face_with_open_mouth_&_closed_eyes': 'ótimo',\n",
        "    'victory_hand': 'ótimo',\n",
        "    'face_with_rolling_eyes': 'ruim',\n",
        "    'revolving_hearts': 'ótimo',\n",
        "    'smiling_face_with_open_mouth': 'ótimo',\n",
        "    'rolling_on_the_floor_laughing': 'ótimo',\n",
        "    'pensive_face': 'ruim',\n",
        "    'dizzy_face': 'ruim',\n",
        "    'angry_face': 'ruim',\n",
        "    'confused_face': 'ruim',\n",
        "    'smiling_face_with_open_mouth_&_cold_sweat': 'ótimo',\n",
        "    'smirking_face': 'ótimo',\n",
        "    'smiling_face_with_sunglasses': 'ótimo',\n",
        "    'face_with_tears_of_joy': 'ótimo',\n",
        "    'white_medium_star': 'ótimo',\n",
        "    'thumbs_down': 'ruim',\n",
        "    'red_heart': 'ótimo',\n",
        "    'clapping_hands': 'ótimo',\n",
        "    'smiling_face_with_halo': 'ótimo',\n",
        "    'purple_heart': 'ótimo',\n",
        "    'smiling_face_with_heart-eyes': 'ótimo',\n",
        "    'heart_suit': 'ótimo',\n",
        "    'hugging_face': 'ótimo',\n",
        "    'glowing_star': 'ótimo',\n",
        "    'smiling_face_with_smiling_eyes': 'ótimo',\n",
        "    'grinning_face_with_smiling_eyes': 'ótimo',\n",
        "    'thumbs_up': 'ótimo',\n",
        "    'face_blowing_a_kiss': 'ótimo',\n",
        "    'winking_face': 'ótimo',\n",
        "    'smiling_face_with_hearts': 'ótimo',\n",
        "    'clown_face': 'ruim'\n",
        "}\n",
        "\n",
        "dict_emotis = {\n",
        "    'Wink or smirk': 'ótimo',\n",
        "    'Happy face or smiley': 'ótimo',\n",
        "    'Tongue sticking out, cheeky, playful or blowing a raspberry': 'ótimo',\n",
        "    'Frown, sad, andry or pouting': 'ruim',\n",
        "    'Skeptical, annoyed, undecided, uneasy or hesitant': 'ruim'\n",
        "}\n",
        "\n",
        "len(dict_emojis), len(dict_emotis)\n",
        "# dos 105 emojis, 71 foram traduzidos. Os emoticons foram todos traduzidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LvY0H4tJqM_8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Pre processing\n",
        "\n",
        "#clean the tweets\n",
        "def pre_processing(text):\n",
        "\n",
        "  #remove special caractheres \n",
        "  pat1 = r'@[^ ]+'                   \n",
        "  pat2 = r'https?://[A-Za-z0-9./]+'  \n",
        "  pat3 = r'\\'s'                      \n",
        "  pat4 = r'\\#\\w+'                     \n",
        "  pat5 = r'&amp '\n",
        "  pat6 = r'(.)\\1{3,}'                              \n",
        "  combined_pat = r'|'.join((pat1, pat2, pat3, pat4, pat5, pat6))\n",
        "  text = re.sub(combined_pat,\"\",text).lower()\n",
        "  text = unidecode.unidecode(text)\n",
        "  text = \"\".join([i for i in text if i not in string.punctuation])\n",
        "\n",
        "  #translate emojis\n",
        "  for emot_obj in UNICODE_EMOJI:\n",
        "    text = text.replace(emot_obj, UNICODE_EMOJI[emot_obj])\n",
        "    text = text.replace(':', ' ')\n",
        "\n",
        "  lista = text.split(' ')\n",
        "  for x in range(len(lista)):\n",
        "      chave = lista[x]\n",
        "      if chave in dict_emojis:\n",
        "          lista[x] = dict_emojis[chave]\n",
        "      if chave in dict_emotis:\n",
        "          lista[x] = dict_emotis[chave]\n",
        "          \n",
        "  text = ' '\n",
        "  text = (text.join(lista))\n",
        "\n",
        "  #remove stopwords\n",
        "  stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "  newStopWords = ['q','vc', 'pq', 'rs', 's', 'fud', 'pra', 'ei', \n",
        "                  'tá', 'vai', 'pa', 've', 'ta', 'é', 'to', 'tb', \n",
        "                  'ir', 'n', 'p', 'ai', 'vei']\n",
        "  stopwords.extend(newStopWords)\n",
        "  palavras = [i for i in text.split() if not i in stopwords]\n",
        "\n",
        "  text = (\" \".join(palavras))\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# Aplicando o stemming em nossa base:\n",
        "def Stemming(text):\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "    palavras = []\n",
        "    for w in text.split():\n",
        "        palavras.append(stemmer.stem(w))\n",
        "    return (\" \".join(palavras))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MWYDDzGHjZKH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title pre processing at bolsonaro DATASET\n",
        "'''\n",
        "pre processing at Bolsonaro dataset\n",
        "\n",
        "'''\n",
        "df1[\"cleaned_tweet\"] = df1[\"tweet\"].apply(lambda x:pre_processing(x))\n",
        "df1['steemer_twitter'] = df1['cleaned_tweet'].apply (lambda x: Stemming(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cggM7tFvdvMY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title pre processing at Lula DATASET\n",
        "'''\n",
        "pre processing in Lula dataset\n",
        "time estimate ~20 minutes\n",
        "'''\n",
        "df2[\"cleaned_tweet\"] = df2[\"tweet\"].apply(lambda x:pre_processing(x))\n",
        "df2['steemer_twitter'] = df2['cleaned_tweet'].apply (lambda x: Stemming(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ni5SgV-hM12b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title pre processing at Ciro DATASET\n",
        "'''\n",
        "pre processing in Ciro Gomes dataset\n",
        "time estimate 1 minute and 24 seconds\n",
        "'''\n",
        "df3[\"cleaned_tweet\"] = df3[\"tweet\"].apply(lambda x:pre_processing(x))\n",
        "df3['steemer_twitter'] = df3['cleaned_tweet'].apply (lambda x: Stemming(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "CiiTeEGujZY7"
      },
      "outputs": [],
      "source": [
        "#@title Save the datasets cleaned in google drive\n",
        "df1.to_csv(PATH_DATA + 'interim' + '/bolsonaro_tratado.csv', sep=';', index=False)\n",
        "df2.to_csv(PATH_DATA + 'interim' + '/lula_tratado.csv', sep=';', index=False)\n",
        "df3.to_csv(PATH_DATA + 'interim' + '/ciro_tratado.csv', sep=';', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0zwOX-U_fD9U",
        "Z7qk4QgPTTb6",
        "PPqI1n8Fy6Y6"
      ],
      "name": "2 tratamento e limpeza.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZ5urCCeYOSfoR1bg2ONGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}